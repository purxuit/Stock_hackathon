{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"polars[all]\"\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sb.set()\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Settings\n",
    "work_dir = \"C:\\\\Users\\\\harim\\\\Downloads\\\\Hackathon\"\n",
    "\n",
    "ret_var = \"stock_ret\"\n",
    "start_date = datetime.date(2005, 1, 1)\n",
    "end_date   = datetime.date(2026, 1, 1)\n",
    "\n",
    "print(\"Imports and settings loaded at\", datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d38e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fac_path = os.path.join(work_dir, \"factor_char_list.csv\")\n",
    "stock_vars = pl.read_csv(fac_path)[\"variable\"].to_list()\n",
    "print(\"Loaded predictors:\", len(stock_vars))\n",
    "print(stock_vars[:10])  # preview first 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(work_dir, \"ret_sample.csv\")\n",
    "raw = pl.read_csv(\n",
    "    csv_path,\n",
    "    try_parse_dates=True,\n",
    "    schema_overrides={\n",
    "        **{v: pl.Float32 for v in stock_vars},  # predictors as floats\n",
    "        \"gvkey\": pl.Utf8, \"iid\": pl.Utf8, \"id\": pl.Utf8,\n",
    "    },\n",
    "    low_memory=True,\n",
    "    rechunk=True,\n",
    ")\n",
    "\n",
    "# Add predictor date (char_date as YYYYMMDD -> pl.Date)\n",
    "raw = raw.with_columns(\n",
    "    pl.col(\"char_date\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y%m%d\").alias(\"date\")\n",
    ")\n",
    "\n",
    "raw = raw.filter(pl.col(ret_var).is_not_null())\n",
    "print(\"Raw shape:\", raw.shape)\n",
    "print(raw.select([\"date\", \"ret_eom\", ret_var]).head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_to_unit(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    out = df\n",
    "    for var in stock_vars:\n",
    "        median = out[var].median()\n",
    "        out = out.with_columns(\n",
    "            pl.when(pl.col(var).is_null()).then(pl.lit(median, dtype=pl.Float32)).otherwise(pl.col(var)).alias(var)\n",
    "        )\n",
    "        out = out.with_columns((pl.col(var).rank(\"dense\") - 1).cast(pl.Float32).alias(var))\n",
    "        maxv = out[var].max()\n",
    "        if maxv is None or maxv == 0:\n",
    "            out = out.with_columns(pl.lit(0.0, dtype=pl.Float32).alias(var))\n",
    "        else:\n",
    "            out = out.with_columns(((pl.col(var) / pl.lit(maxv, dtype=pl.Float32)) * 2.0 - 1.0).alias(var))\n",
    "    return out\n",
    "\n",
    "data = raw.group_by(\"date\", maintain_order=True).map_groups(rank_to_unit)\n",
    "print(\"Rank-transformed shape:\", data.shape)\n",
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9091e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# ✅ Choose top predictors (replace with your chosen subset)\n",
    "selected_predictors = [\"chcsho_12m\",\"rd_me\", \"at_me\", \"fcf_me\", \"ope_be\", \"turnover_126d\", \"eqnpo_12m\", \"rd_sale\", \"bidaskhl_21d\"]  # example\n",
    "columns_to_keep = [\"date\", \"gvkey\", \"iid\", \"id\", ret_var] + selected_predictors\n",
    "\n",
    "# ✅ Keep only chosen columns\n",
    "cleaned = raw.select([c for c in columns_to_keep if c in raw.columns])\n",
    "\n",
    "# ✅ Drop rows with nulls in these columns\n",
    "cleaned = cleaned.drop_nulls()\n",
    "\n",
    "# ✅ Save cleaned dataset to a new CSV\n",
    "output_path = os.path.join(work_dir, \"cleaned_data.csv\")\n",
    "cleaned.write_csv(output_path)\n",
    "\n",
    "print(\"Cleaned data shape:\", cleaned.shape)\n",
    "print(\"Saved cleaned file to:\", output_path)\n",
    "print(cleaned.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6810e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_frames = []\n",
    "counter = 0\n",
    "\n",
    "max_data_date = data.select(pl.col(\"date\").max()).item()\n",
    "safe_end = min(end_date, (max_data_date - datetime.timedelta(days=365)))\n",
    "\n",
    "while (start_date + datetime.timedelta(days=365*(11 + counter))) <= safe_end:\n",
    "    cutoff = [\n",
    "        start_date,\n",
    "        start_date + datetime.timedelta(days=365*(8  + counter)),\n",
    "        start_date + datetime.timedelta(days=365*(10 + counter)),\n",
    "        start_date + datetime.timedelta(days=365*(11 + counter)),\n",
    "    ]\n",
    "    print(\"Window\", counter, cutoff)\n",
    "\n",
    "    train = data.filter((pl.col(\"date\") >= cutoff[0]) & (pl.col(\"date\") < cutoff[1]))\n",
    "    val   = data.filter((pl.col(\"date\") >= cutoff[1]) & (pl.col(\"date\") < cutoff[2]))\n",
    "    test  = data.filter((pl.col(\"date\") >= cutoff[2]) & (pl.col(\"date\") < cutoff[3]))\n",
    "\n",
    "    if train.height == 0 or val.height == 0 or test.height == 0:\n",
    "        print(\"Empty window, skipping\")\n",
    "        counter += 1\n",
    "        continue\n",
    "\n",
    "    # Convert to numpy\n",
    "    X_train = train.select(stock_vars).to_numpy()\n",
    "    Y_train = train.select(ret_var).to_numpy().ravel()\n",
    "    X_val   = val.select(stock_vars).to_numpy()\n",
    "    Y_val   = val.select(ret_var).to_numpy().ravel()\n",
    "    X_test  = test.select(stock_vars).to_numpy()\n",
    "    Y_test  = test.select(ret_var).to_numpy().ravel()\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train, X_val, X_test = scaler.transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n",
    "\n",
    "    Y_mean = Y_train.mean()\n",
    "    Y_dm   = Y_train - Y_mean\n",
    "\n",
    "    reg_pred = test.select([\"year\", \"month\", \"ret_eom\", \"id\", ret_var]).to_pandas()\n",
    "\n",
    "    # Models\n",
    "    reg = LinearRegression(fit_intercept=False).fit(X_train, Y_dm)\n",
    "    reg_pred[\"ols\"] = reg.predict(X_test) + Y_mean\n",
    "\n",
    "    # lambdas = np.arange(-4, 4.1, 0.1)\n",
    "    # val_mse = [mean_squared_error(Y_val, Lasso(alpha=10**p, max_iter=1_000_000, fit_intercept=False).fit(X_train, Y_dm).predict(X_val) + Y_mean) for p in lambdas]\n",
    "    # best = lambdas[int(np.argmin(val_mse))]\n",
    "    # reg_pred[\"lasso\"] = Lasso(alpha=10**best, max_iter=1_000_000, fit_intercept=False).fit(X_train, Y_dm).predict(X_test) + Y_mean\n",
    "\n",
    "    lambdas = np.arange(-1, 8.1, 0.1)\n",
    "    val_mse = [mean_squared_error(Y_val, Ridge(alpha=(10**p)*0.5, fit_intercept=False).fit(X_train, Y_dm).predict(X_val) + Y_mean) for p in lambdas]\n",
    "    best = lambdas[int(np.argmin(val_mse))]\n",
    "    reg_pred[\"ridge\"] = Ridge(alpha=(10**best)*0.5, fit_intercept=False).fit(X_train, Y_dm).predict(X_test) + Y_mean\n",
    "\n",
    "    # lambdas = np.arange(-4, 4.1, 0.1)\n",
    "    # val_mse = [mean_squared_error(Y_val, ElasticNet(alpha=10**p, max_iter=1_000_000, fit_intercept=False).fit(X_train, Y_dm).predict(X_val) + Y_mean) for p in lambdas]\n",
    "    # best = lambdas[int(np.argmin(val_mse))]\n",
    "    # reg_pred[\"en\"] = ElasticNet(alpha=10**best, max_iter=1_000_000, fit_intercept=False).fit(X_train, Y_dm).predict(X_test) + Y_mean\n",
    "\n",
    "    pred_frames.append(reg_pred)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Finished backtest loop with\", counter, \"windows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load the cleaned CSV created earlier\n",
    "csv_path = os.path.join(work_dir, \"cleaned_data.csv\")\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Summary Stats\n",
    "# -------------------------------\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(df.describe(include=\"all\"))\n",
    "\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(df.isnull().sum().sort_values(ascending=False))\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Distribution of Stock Returns\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(df[ret_var], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Stock Returns\")\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Correlation Heatmap\n",
    "# -------------------------------\n",
    "predictors = [col for col in df.columns if col not in [\"date\", \"gvkey\", \"iid\", \"id\", ret_var]]\n",
    "plt.figure(figsize=(12,8))\n",
    "corr = df[predictors + [ret_var]].corr()\n",
    "sns.heatmap(corr, annot=False, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap (Predictors vs Return)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Top Correlated Predictors\n",
    "# -------------------------------\n",
    "corr_with_return = corr[ret_var].drop(ret_var).sort_values(key=abs, ascending=False)\n",
    "print(\"\\n=== Top Predictors Correlated with Return ===\")\n",
    "print(corr_with_return.head(10))\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Pairplot (Predictors vs Return)\n",
    "# -------------------------------\n",
    "sns.pairplot(df[[ret_var] + predictors[:4]], diag_kind=\"kde\")  # first few vars\n",
    "plt.suptitle(\"Pairwise Plots (Returns vs Predictors)\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Time Series of Returns\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "df.groupby(\"date\")[ret_var].mean().plot()\n",
    "plt.title(\"Average Stock Return Over Time\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Scatter Plots for Key Predictors\n",
    "# -------------------------------\n",
    "top_vars = corr_with_return.head(3).index.tolist()  # top 3 predictors\n",
    "for v in top_vars:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(x=df[v], y=df[ret_var], alpha=0.5)\n",
    "    plt.title(f\"{v} vs Stock Return\")\n",
    "    plt.xlabel(v)\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Predictive Modeling (OLS, Ridge, etc.)\n",
    "# -------------------------------\n",
    "\n",
    "# Extract predictors (X) and target (y)\n",
    "X = df[predictors].values\n",
    "y = df[ret_var].values\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train/test set\n",
    "train_size = int(0.8 * len(df))\n",
    "X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# 1. OLS\n",
    "ols_model = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n",
    "ols_pred = ols_model.predict(X_test)\n",
    "ols_r2 = 1 - np.sum((y_test - ols_pred)**2) / np.sum((y_test - y_test.mean())**2)\n",
    "print(\"\\nOLS Model R²:\", ols_r2)\n",
    "\n",
    "# 2. Ridge Regression (hyperparameter tuning)\n",
    "ridge_model = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "ridge_pred = ridge_model.predict(X_test)\n",
    "ridge_r2 = 1 - np.sum((y_test - ridge_pred)**2) / np.sum((y_test - y_test.mean())**2)\n",
    "print(\"Ridge Model R²:\", ridge_r2)\n",
    "\n",
    "# 3. Lasso Regression (hyperparameter tuning)\n",
    "lasso_model = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "lasso_pred = lasso_model.predict(X_test)\n",
    "lasso_r2 = 1 - np.sum((y_test - lasso_pred)**2) / np.sum((y_test - y_test.mean())**2)\n",
    "print(\"Lasso Model R²:\", lasso_r2)\n",
    "\n",
    "# 4. ElasticNet Regression (hyperparameter tuning)\n",
    "en_model = ElasticNet(alpha=0.1, l1_ratio=0.5).fit(X_train, y_train)\n",
    "en_pred = en_model.predict(X_test)\n",
    "en_r2 = 1 - np.sum((y_test - en_pred)**2) / np.sum((y_test - y_test.mean())**2)\n",
    "print(\"ElasticNet Model R²:\", en_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a5649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6ccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
